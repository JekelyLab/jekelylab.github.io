---
title: "Open Science in (In)Action: Why, What, and How"
author: "Gáspár Jékely <br> Centre for Organismal Studies, Heidelberg University"
format:
  revealjs: 
    theme: [default, custom.scss]
    slide-number: true
    code-fold: false
    code-summary: "Code"
    chalkboard: 
      buttons: false
    preview-links: auto
scrollable: true
---

## 

![](images/open_science/2025-V4SDBWinterSchool_OpenScience_lecture.png){height=650}

## UNESCO Recommendation on Open Science

![](images/open_science/UNESCO_opne_science.png){height=550}

:::footer
[UNESCO](https://unesdoc.unesco.org/ark:/48223/pf0000379949)
:::


## What is open science?


- data are shared, indexed, properly archived and discoverable
- same for scientific code
- analyses and workflows to process data are also shared
- reagents (e.g. plasmids), strains (e.g. mutants) shared
- open-source software
- maximise reproducibility
- text + data + code = publication
- publications openly accessible, not behind a paywall
- publishing is affordable and not distorting (not hijacked by corporate for-profit publishers)
- preprints = publication, followed by post-publication peer review

## The Budapest Open Access Initiative

::: columns
:::{.column width=70%}
![](images/open_science/BOAI.png)
- https://www.budapestopenaccessinitiative.org/
<br>*February 14, 2002*
<br>Budapest, Hungary

::: 
:::{.column width=30%}

- removing access barriers to literature
- free and unrestricted online availability = **open access**
- *experiments show that the overall costs of providing open access to this literature are far lower than the costs of traditional forms of dissemination*
- opportunity to save money and expand the scope of dissemination 
- recommendations: **self-archiving** (I.) and a new generation of **open-access journals** (II.)
:::
:::


## The **FAIR** Guiding Principles for scientific data management and stewardship


::: columns
:::{.column width=70%}
![](images/open_science/FAIR_principles.png)
::: 
:::{.column width=30%}
- emphasis on ability to programmatically find and use the data
- supporting its reuse
- accurare description
- https://www.go-fair.org/fair-principles/
:::
:::

:::footer
[Wilkinson et al. 2016](https://www.nature.com/articles/sdata201618)
:::

## One example - the Uniprot database

::: columns
:::{.column width=70%}
![](images/open_science/uniprot.png)
::: 
:::{.column width=30%}
- a comprehensive resource for protein sequence and annotation data
- entries uniquely identified by a stable URL
- rich metadata that is both human-readable and machine-readable
- shared vocabularies and ontologies
- interlinking with more than 150 different databases
:::
:::

:::footer
[UniProt Consortium, 2015](https://doi.org/10.1093/nar/gku989)
:::

## Why do we need open science?

- Reproducibility crisis
- Only a small fraction of research data is available
- An even smaller fraction of code is available (physicists are notoriously bad in sharing)
- Open access, if exists, is very expensive and maintains the profit of legacy publishers
- Scholarly literature is antiquated, dysfunctional and rewards prestige/hype over quality and integrity
- Scholarly workflows use non-professional, error-prone, closed-source software (MS, Adobe, Prism etc.) that makes sharing, integration, automation and collaboration difficult
- The final product of years of research is often only a single pdf file (1990s tech) behind a paywall
- Data, code and text are not searchable, reuseable, discoverable, shareable

## Most source data collected by scientists are not available

<br>

![](images/Data_publication_Pyramide.png){height=450}

## Majority high-impact cancer studies fail to replicate

::: columns
:::{.column width=50%}
![](images/open_science/cancer_replication.png)
::: 
:::{.column width=50%}
- The Reproducibility Project: Cancer Biology (RP:CB) 
- failure to replicate 30 of 53 papers published by *Science*, *Nature*, and *Cell* from 2010 to 2012
- credibility of preclinical cancer biology?
- need for authors to share more details of their experiments
- vague protocols and uncooperative authors
- one-third of contacted authors declined or did not respond

:::
:::

:::footer
[Errington et al., Reproducibility Project: Cancer Biology, 2014](https://elifesciences.org/collections/9b1e83d1/reproducibility-project-cancer-biology)
:::


## Code is very often not shared or not shared stably

- *We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science ... We found that we were able to obtain artifacts from 44% of our sample and were able to reproduce the findings for 26%.*

::: {.incremental}
- "When you approach a PI for the source codes and raw data, you better explain who you are, whom you work for, why you need the data and what you are going to do with it."

- "I have to say that this is a very unusual request without any explanation! Please ask your supervisor to send me an email with a detailed, and I mean detailed, explanation."

- "The data files remains our property and are not deposited for free access. Please, let me know the purpose you want to get the file and we will see how we can help you."

- "We do not typically share our internal data or code with people outside our collaboration."

:::

![](images/Stodden_PNAS.png){.absolute top="480" left="300" height="200"}

::: aside
[Stodden et al. (2018) ](https://doi.org/10.1073/pnas.1708290115)
:::

## Flipped protein structures due to buggy program


::: columns
:::{.column width=40%}
![](images/open_science/MsbA_flipped.png)

*The structures of MsbA (purple) and Sav1866 (green) overlap little (left) until MsbA is inverted (right).*
:::
:::{.column width=60%}
- buggy non-published program flipped two columns, inverting electron density
- program was inherited from another lab
- mistake repeated in several papers
- led to five retractions (three in Science)
:::
:::

:::footer
[Miller 2006](https://www.science.org/doi/10.1126/science.314.5807.1856)
:::


## Gene name errors are widespread in the scientific literature

<br><br>

![](images/13059_2016_1044_Fig1_HTML.webp)


::: aside
[Ziemann et al. (2016)](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1044-7)
:::

## Most scientists use software developed for accounting {transition="fade" transition-speed="fast"}

![](images/Scientists_rename_genes.png)
[theverge.com](https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates)

- the symbol MARCH1 has now become MARCHF1, while SEPT1 has become SEPTIN1, and so on

<br>


## Current state of scholarly digital infrastructure and knowhow

<img src=https://media1.tenor.com/images/f6362876996697b6a6f554b2ac3d3013/tenor.gif?itemid=10488408 width=160>



## An epidemic of retractions

<br><br>

::: columns
:::{.column width=50%}
![](images/open_science/retractions.png)
::: 
:::{.column width=50%}
- exponential increase in retractions
- monitoring retractions: http://retractionwatch.com
- majority of all retractions is due to misconduct
:::
:::

:::footer
[Brembs et al. 2013](https://doi.org/10.3389/fnhum.2013.00291)
:::

## Perverse incentives, publish or perish

::: columns
:::{.column width=60%}
![](images/open_science/adoration_CNS.png)
::: 
:::{.column width=40%}
- chasing titles, 'stories' and IF instead of scientific integrity, hypothesis-testing, rigour, openness
- under the spell of glamour journals
- "**If** I get this result, this will be a *Nature* paper!"
- reporting bias (only positive results are reported)
- low statistical power (a p-value of 0.05 brings only 50% reproducibility)
- in worst cases data are fabricated

:::
:::

:::footer
[after Brembs]
:::


## Impact factor - not a metric of quality

- IF = number of citations to articles in a journal (the numerator), normalized by the number of articles in that journal (the denominator) in the last 2 y
- calculated by Thomson Reuters
- originally created to help librarians identify journals to purchase, not as a measure of the scientific quality of research in an article
- yet, emerged as a pervasive metric of quality
- in some cases not calculated but negotiated (the denominator)
- removing editorials and News-and-Views articles from the denominator (so called “front-matter”) can dramatically alter the resulting IF
- not reproducible, not open (calculated from proprietary data)
- a composite of multiple, highly diverse article types
- comparison of journals not mathematically sound

:::footer
[Brembs et al. 2013](https://doi.org/10.3389/fnhum.2013.00291)
:::

## IF - statistically flawed

::: columns
:::{.column width=60%}
![](images/open_science/if_plots.png)
::: 
:::{.column width=40%}
- highly skewed distributions
- journal IF comparisons: comparison of means of two populations 
- allowed only if distributions follow a **normal distributions**! 
- simple ranking by mean is incorrect
- median would be better or a more complex statistical test (e.g. Kruskal–Wallis test)
:::
:::

:::footer
[Larivière et al. 2016](https://doi.org/10.1101/062109)
:::

## IF - strongly biased by outliers

::: columns
:::{.column width=50%}
![](images/open_science/if_model_fit.png)
::: 
:::{.column width=50%}
<br>

- fitting a more complex exponential function to the citation data
- a journal impact factor can be calculated from the parameters of the fit
- *Science* JIF = **25.3** instead of the reported **34**
- *Nature* JIF = **26.8** instead of the reported **37**
- a few highly cited papers have a substantial effect on the mean, but less on the exponential function
:::
:::

:::footer
[Berg, 2016](https://www.science.org/content/blog-post/journal-impact-factors---fitting-citation-distribution-curves)
:::

## JIF does not correlate with quality metrics (e.g. statistical power)

::: columns
:::{.column width=50%}
![](images/open_science/fnhum-07-00291-g002.jpg)
::: 
:::{.column width=50%}
- no association between statistical power and journal IF
:::
:::

:::footer
[Fang & Casadevall, 2011](https://journals.asm.org/doi/10.1128/iai.05661-11)
[Brembs et al. 2013](https://doi.org/10.3389/fnhum.2013.00291)
:::

## But: JIF correlates with retractions

::: columns
:::{.column width=50%}
![](images/open_science/retraction_IF.jpeg)
::: 
:::{.column width=50%}
- journal rank is a strong predictor of the rate of retractions
- (also of Excel errors :))
:::
:::

:::footer
[Fang & Casadevall, 2011](https://journals.asm.org/doi/10.1128/iai.05661-11)
[Brembs et al. 2013](https://doi.org/10.3389/fnhum.2013.00291)
:::

## New approaches to research assessment

<br>

::: columns
:::{.column width=60%}

![](images/open_science/dora.png)
::: 
:::{.column width=40%}
- the San Francisco Declaration on Research Assessment
- eliminate the use of journal-based metrics, such as Journal Impact Factors, in funding, appointment, and promotion considerations
- assess research on its own merits rather than on the basis of the journal in which the research is published
- capitalize on the opportunities provided by online publication (e.g. relax limits on the number of words, figures, and references)
:::
:::

:::footer
[DORA](https://sfdora.org/read/)
:::

## Publications categorised by type of access

::: columns
:::{.column width=60%}
![](images/open_science/publications_by_OA_category.png)
::: 
:::{.column width=40%}
- Diamond: article published in an open access (only) journal without an article processing charge
- Green only: not openly accessible from the publisher website but a free copy is accessible via a repository or other platform
- Gold: article published in an open access (only) journal that includes article processing charges
- Hybrid
- Bronze: free to read on the publisher’s website, but with no identifiable licence
:::
:::

:::footer
[UNESCO - Open Science Outlook 1](https://unesdoc.unesco.org/ark:/48223/pf0000387324)
:::

## Current system is hugely wasteful

::: columns
:::{.column width=40%}
![](images/open_science/Maxwell.avif)
Robert Maxwell in 1985. *Photograph: Terry O’Neill/Hulton/Getty*
::: 
:::{.column width=60%}
- worldwide sales > USD 19 billion
- dominated by five large publishing houses: Elsevier, Black & Wiley, Taylor & Francis, Springer Nature and SAGE
- Elsevier has a profit margin around 40 % (higher than Microsoft, Google and Coca Cola)
- about **USD 6 billion per year** goes to profits = 2 CERNs/year 
:::
:::

:::footer
[Hagve, 2015](https://tidsskriftet.no/en/2020/08/kronikk/money-behind-academic-publishing)
[Buranyi 2017](https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science)
:::

## Kleptistan (Bindzsisztán)

::: columns
:::{.column width=80%}
![](images/open_science/Majka_Csurran_Cseppen.webm)
:::
:::{.column width=20%}

- Elsevierstan
- Wileystan
- Taylorfrancistan
- Springerstan
- ...
- an oligopoly of legacy publishers
:::
:::

:::footer
[Majka](https://www.youtube.com/watch?app=desktop&v=f2iQfEcO39A)
:::

## We have the solution, but not the balls to implement it

<br>

- under ‘closed’ models, institutions are already spending money on scientific publications 
- transitioning those funds to support diamond open access publication could fully support a global shift to open access publishing 
- at the same time strenghten scholarly infrastructure for code, data, interoperability etc.
- with the potential for cost savings (Schimmer et al. 2015)
- with publishing in the hand of public institutions
- USD 6 billion/year is a lot of money for that


:::footer
[UNESCO - Open Science Outlook 1](https://unesdoc.unesco.org/ark:/48223/pf0000387324)
:::


## Open Science under attack

::: columns
:::{.column width=50%}
<br>

![](images/open_science/elsevier.png)
::: 
:::{.column width=50%}
<br><br>

- overt commercial predation
- monetization of academic research output
- predatory journals and conferences
- the falsification of experimental evidence
- fake qualifications, certificates and awards 
- predatory preprint servers
- surveillance by legacy publishers (e.g. Elsevier)
:::
:::

:::footer
[UNESCO - Open Science Outlook 1](https://unesdoc.unesco.org/ark:/48223/pf0000387324)
:::


## Open Science under attack

::: columns
:::{.column width=70%}
<br>

![](images/open_science/nasa_open_science.png)
::: 
:::{.column width=30%}
<br><br>

- As recently as January 2, 2025, NASA had five modules for teaching Open Science on its web site. 
- The Trump administration has taken them down
:::
:::

:::footer
[@internetarchive Wayback Machine](https://web.archive.org/web/20250102141453/https://science.nasa.gov/open-science/tops/os101/)
:::

##

![](images/open_science/nasa_404.png){height=650}

## Major repositories and databases in constant danger

![](images/open_science/pubmed_shutdown.jpg)

![](images/open_science/funding_for_NIH_databases.png)


## Taking back control

::: columns
:::{.column width=40%}
![](images/open_science/openaire.png)
::: 
:::{.column width=60%}
- Public institutions (universities, libraries, funders etc.) should take back control of the digital scholarly infrastructure
- create conditions of open competition for private sector (not oligopoly of a few publishers)
- control data, text, code, citation metrics, scholarly workflows, databases, standards etc.
- cancel all subscriptions and use money to fund databases, libraries, publishing etc.
- support initiatives like OpenAIRE
:::
:::

:::footer
[OpenAIRE](https://www.openaire.eu/)
:::

## Open Alex

::: columns
:::{.column width=60%}
![](images/open_science/openalex.png)

::: 
:::{.column width=40%}
- https://openalex.org/
- indexes over 250M scholarly works
- nlamed after the ancient Library of Alexandria 
- export all search results for free
- use of API or download the whole dataset
- share and reuse as you like
- made by OurResearch, a nonprofit dedicated to making research open
- 100% of source code is open
:::
:::

## eLife

::: columns
:::{.column width=60%}
![](images/open_science/elife.png)

::: 
:::{.column width=40%}
- https://elifesciences.org/
- **The eLife process has five steps:**
  - Submission or transfer of a preprint from bioRxiv 
  - Peer review (eLife editors - who are all active researchers - discuss new submissions and decide which will be peer reviewed)
  - Publication of Reviewed Preprint
  - Publication of revised version
  - Publication of Version of Record
  - papers published together   with eLife Assessment
  
:::
:::


## EBI Bioimage Archive

::: columns
:::{.column width=65%}
![](images/open_science/bioimage_archiv.png)

- https://www.ebi.ac.uk/bioimage-archive/

- free, public resource for biological images
- funded by UKRI and EMBL member state funding
::: 
:::{.column width=35%}
![](images/open_science/41592_2018_195_Fig1_HTML.webp)

:::
:::

:::footer
[Ellemberg et al. 2018](https://www.nature.com/articles/s41592-018-0195-8)
:::

## What is the solution? - The Fediverse for Science

::: columns
:::{.column width=30%}
![](images/open_science/fediverse.png)
::: 
:::{.column width=70%}
<br>

- a federated infrastructure
- run by public institutions (universities, libraries etc.)
- for communication (microblogging = Mastodon)
- for code (GitLab), data (e.g. Omero), text (preprint servers) etc.
- taking back control of scholarly infrastructure
:::
:::

:::footer
[Imagec credit: @FediTips](@FediTips@social.growyourown.services)
:::

## Towards a new, federated scholarly infrastructure

::: columns
:::{.column width=50%}
![](images/open_science/rsos230206f03.jpg)
::: 
:::{.column width=50%}
- plan for a federated scholarly information network
- a system that cannot be taken over by corporations
- designed redundantly
- open standards
- "a decentralized, resilient, evolvable network that is interconnected by open standards and open-source norms under the governance of the scholarly community"

:::
:::

:::footer
[Brembs et al. 2023](https://royalsocietypublishing.org/doi/full/10.1098/rsos.230206)
:::


## One example for publishing -  Open Research Europe (ORE)

::: columns
:::{.column width=60%}
![](images/open_science/ORE.png)

- open access publishing venue for EC-funded researchers
- no author or reader fees
- Diamond (but authors need to be EC funded)
- maintained by the European Commission
- Wellcome Open Research (https://wellcomeopenresearch.org/) is similar, maintained by the Wellcome Trust
::: 
:::{.column width=30%}
<br>

- open up access methods, results, publications, data, software, materials, tools and peer reviews
- standard tender process held regularly
- no lock-in with a single publisher
- regular procurement processes, no monopoly, fair prices
:::
:::

:::footer
[]()
:::


## Sharing code in an ideal world - federated GitLab servers

<br>

::: columns
:::{.column width=60%}


![](images/open_science/git_on_the_server.png)
::: 
:::{.column width=40%}

- Institutions should host their **GitLab server** for code
- (**GitLab** is a database-backed web application running *git*)
- (*git* is a Distributed Version Control Systems)
- servers should be federated
- European (-> world-wide) network of research/education institutions and libraries
- code shared upon publication in a permanent repo with DOI
:::
:::

:::footer
[Chacon & Straub - Pro Git Book](https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F)
:::


## Code with persistend DOI

<br>

::: columns
:::{.column width=60%}


![](images/open_science/zenodo.png)
::: 
:::{.column width=40%}

- Permanent repository for data, text and code
- integration with **GitHub**
- version control
- **Safe** — your research is stored safely for the future in CERN’s Data Centre for as long as CERN exists
- citeable
- usage statistics
:::
:::



## An European Infrastructure for Open Science

::: columns
:::{.column width=60%}
<br>

![](images/open_science/EOSC_EU_Node.png)
https://open-science-cloud.ec.europa.eu/
::: 
:::{.column width=40%}
- **Available Services:**
  - File Sync & Share 
  - Interactive Notebooks 
  - Large File Transfer 
  - Virtual Machines 
  - Cloud Container Platform 
  - Bulk Data Transfer 
:::
:::



## ...still early days


![](images/open_science/eu_open_science.png)


## Summary

::: columns
:::{.column width=40%}
![](images/open_science/)
::: 
:::{.column width=60%}
- open science is key for reproducibility, scientific progress, integrity, global fairness, resilience, cost-cutting
- solutions exist and are scaleable
- money could come by scaling back payment to legacy publishers
- problems: inertia, lack of coordination, lack of understanding the landscape, political and corporate takeover, prestige and other incentives to maintain status quo
:::
:::

:::footer
[]()
:::


